---
phase: 05-import-pipelines-and-fixture-data
plan: 03
type: execute
wave: 2
depends_on: ["05-01", "05-02"]
files_modified:
  - backend/app/seed/initial_data.py
  - backend/app/api/powders.py
  - backend/app/api/bullets.py
  - backend/app/api/cartridges.py
  - backend/tests/test_import_pipelines.py
autonomous: true
requirements: [PWD-01, PWD-05, BUL-04, CRT-03]

must_haves:
  truths:
    - "After fresh Docker boot, the database contains 200+ powders, 100+ bullets, and 50+ cartridges loaded from fixture files"
    - "User can batch-import GRT powder XML with 3-mode collision handling (skip/overwrite/merge) and user-created records are never overwritten"
    - "User can batch-import bullets from JSON via POST /bullets/import with skip/overwrite/merge collision modes"
    - "User can batch-import cartridges from JSON via POST /cartridges/import with skip/overwrite/merge collision modes"
    - "Powder alias groups are applied during seed loading from powder_aliases.json"
    - "Quality scores and caliber families are computed for all seeded records"
  artifacts:
    - path: "backend/app/seed/initial_data.py"
      provides: "Fixture-based seed loading with quality scoring and alias group application"
      contains: "_load_fixture"
    - path: "backend/app/api/powders.py"
      provides: "Updated GRT import with 3-mode collision handling"
      contains: "ImportMode"
    - path: "backend/app/api/bullets.py"
      provides: "POST /bullets/import batch endpoint"
      contains: "import_bullets"
    - path: "backend/app/api/cartridges.py"
      provides: "POST /cartridges/import batch endpoint"
      contains: "import_cartridges"
    - path: "backend/tests/test_import_pipelines.py"
      provides: "Tests for import endpoints and seed loading"
      min_lines: 200
  key_links:
    - from: "backend/app/seed/initial_data.py"
      to: "backend/app/seed/fixtures/powders.json"
      via: "json.load reads fixture file during seed"
      pattern: "_load_fixture.*powders\\.json"
    - from: "backend/app/api/powders.py"
      to: "backend/app/schemas/powder.py"
      via: "Import endpoint uses ImportMode enum and ImportResult schema"
      pattern: "ImportMode|ImportResult"
    - from: "backend/app/api/bullets.py"
      to: "backend/app/schemas/bullet.py"
      via: "Import endpoint uses BulletImportRequest schema"
      pattern: "BulletImportRequest"
    - from: "backend/app/api/cartridges.py"
      to: "backend/app/schemas/cartridge.py"
      via: "Import endpoint uses CartridgeImportRequest schema"
      pattern: "CartridgeImportRequest"
---

<objective>
Implement batch import endpoints for all three entity types (powders, bullets, cartridges) with 3-mode collision handling, refactor the seed system to load from JSON fixtures on first boot, and add comprehensive tests.

Purpose: Connects the schema changes (Plan 01) with the fixture data (Plan 02) to deliver the full Phase 5 feature set: production-ready pre-loaded databases and batch import capability for future expansion.
Output: Updated seed loader, 3 import endpoints, and test suite.
</objective>

<execution_context>
@C:/Users/vall-/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/vall-/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-import-pipelines-and-fixture-data/05-RESEARCH.md
@.planning/phases/05-import-pipelines-and-fixture-data/05-01-SUMMARY.md
@.planning/phases/05-import-pipelines-and-fixture-data/05-02-SUMMARY.md

Key existing files:
@backend/app/seed/initial_data.py
@backend/app/api/powders.py
@backend/app/api/bullets.py
@backend/app/api/cartridges.py
@backend/app/core/quality.py
@backend/app/services/search.py
@backend/tests/test_api_integration.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor seed system and upgrade powder import endpoint</name>
  <files>
    backend/app/seed/initial_data.py
    backend/app/api/powders.py
  </files>
  <action>
**Refactor `initial_data.py`** to load from JSON fixtures instead of inline Python dicts:

1. Add `import json` and `from pathlib import Path` imports.
2. Define `FIXTURES_DIR = Path(__file__).parent / "fixtures"`.
3. Add helper: `def _load_fixture(filename: str) -> list | dict: filepath = FIXTURES_DIR / filename; return json.load(open(filepath)) if filepath.exists() else []`
4. Remove the inline `POWDERS`, `BULLETS`, `CARTRIDGES` lists (replace with fixture loading).
5. Keep `RIFLES` list inline (only 5 records, tied to cartridge_name references).

Rewrite `seed_initial_data(db)`:
```python
async def seed_initial_data(db: AsyncSession):
    """Load fixture data on first boot if tables are empty."""
    existing = await db.execute(select(Powder).limit(1))
    if existing.scalar():
        logger.info("Seed data already exists, skipping")
        return

    logger.info("Seeding fixture data from JSON fixtures...")

    # ---- Powders ----
    powders_data = _load_fixture("powders.json")
    powder_objects = []
    for data in powders_data:
        # Filter to only known Powder model columns
        powder = Powder(**{k: v for k, v in data.items() if hasattr(Powder, k) and k != "id"})
        if not powder.data_source:
            powder.data_source = "estimated"
        # Compute quality score
        powder_dict = {k: v for k, v in data.items()}
        breakdown = compute_quality_score(powder_dict, powder.data_source)
        powder.quality_score = breakdown.score
        powder_objects.append(powder)
    db.add_all(powder_objects)

    # Apply alias groups from separate file
    aliases_data = _load_fixture("powder_aliases.json")
    if aliases_data and isinstance(aliases_data, dict):
        await db.flush()  # ensure powder names are in session
        for group_name, powder_names in aliases_data.items():
            for pname in powder_names:
                result = await db.execute(select(Powder).where(Powder.name == pname))
                p = result.scalar_one_or_none()
                if p:
                    p.alias_group = group_name

    # ---- Bullets ----
    bullets_data = _load_fixture("bullets.json")
    bullet_objects = []
    for data in bullets_data:
        bullet = Bullet(**{k: v for k, v in data.items() if hasattr(Bullet, k) and k != "id"})
        if not bullet.data_source:
            bullet.data_source = "manufacturer"
        bullet.caliber_family = derive_caliber_family(bullet.diameter_mm)
        bullet_dict = {k: v for k, v in data.items()}
        breakdown = compute_bullet_quality_score(bullet_dict, bullet.data_source)
        bullet.quality_score = breakdown.score
        bullet_objects.append(bullet)
    db.add_all(bullet_objects)

    # ---- Cartridges ----
    cartridges_data = _load_fixture("cartridges.json")
    cartridge_objects = []
    for data in cartridges_data:
        cart = Cartridge(**{k: v for k, v in data.items() if hasattr(Cartridge, k) and k != "id"})
        if not cart.data_source:
            cart.data_source = "saami"
        cart.caliber_family = derive_caliber_family(cart.groove_diameter_mm)
        cart_dict = {k: v for k, v in data.items()}
        breakdown = compute_cartridge_quality_score(cart_dict, cart.data_source)
        cart.quality_score = breakdown.score
        cartridge_objects.append(cart)

    # Build cartridge map for rifle FK references
    cartridge_map = {c_data["name"]: cart for c_data, cart in zip(cartridges_data, cartridge_objects)}
    db.add_all(cartridge_objects)

    await db.flush()  # assign IDs to cartridges before creating rifles

    # ---- Rifles (inline, only 5) ----
    for data in RIFLES:
        cart = cartridge_map.get(data["cartridge_name"])
        if not cart:
            logger.warning("Cartridge '%s' not found for rifle '%s', skipping", data["cartridge_name"], data["name"])
            continue
        rifle = Rifle(name=data["name"], barrel_length_mm=data["barrel_length_mm"],
                      twist_rate_mm=data["twist_rate_mm"], cartridge_id=cart.id,
                      chamber_volume_mm3=data["chamber_volume_mm3"],
                      weight_kg=data.get("weight_kg", 3.5))
        db.add(rifle)

    await db.commit()
    logger.info("Fixture data seeded: %d powders, %d bullets, %d cartridges, %d rifles",
                len(powder_objects), len(bullet_objects), len(cartridge_objects), len(RIFLES))
```

Import the required quality and search functions at the top of the function or at module level.

**Upgrade `powders.py` import endpoint** from simple `overwrite: bool` to 3-mode `ImportMode`:

1. Import `ImportMode` and `ImportResult` from `app.schemas.powder`.
2. Replace the existing `import_grt` endpoint signature:
   - Change `overwrite: bool = False` to `mode: ImportMode = Query(ImportMode.skip)`
3. Update collision handling logic:
   - If `existing.data_source == "manual"` AND mode is overwrite or merge: NEVER overwrite. Instead, rename the imported powder to `"{name} (GRT Import)"` and create as new record. Add to `created` list.
   - If `mode == ImportMode.skip`: skip (existing behavior).
   - If `mode == ImportMode.overwrite`: update ALL fields on existing record (existing behavior when overwrite=True). Set data_source to "grt_community".
   - If `mode == ImportMode.merge`: only update fields that are currently NULL on the existing record. Do NOT overwrite non-null values. Leave data_source unchanged.
4. Return updated `GrtImportResult` with the `updated` list populated for overwrite/merge operations.
5. Also add a `GET /powders/{powder_id}/aliases` endpoint that returns other powders with the same alias_group:
   ```python
   @router.get("/{powder_id}/aliases", response_model=list[PowderResponse])
   async def get_powder_aliases(powder_id: uuid.UUID, db: AsyncSession = Depends(get_db)):
       powder = await db.get(Powder, powder_id)
       if not powder:
           raise HTTPException(404, "Powder not found")
       if not powder.alias_group:
           return []
       result = await db.execute(
           select(Powder).where(Powder.alias_group == powder.alias_group, Powder.id != powder_id)
       )
       return list(result.scalars().all())
   ```
  </action>
  <verify>
1. Read `initial_data.py` and confirm it loads from JSON fixtures via `_load_fixture()`, computes quality scores, applies alias groups, and preserves RIFLES inline
2. Read `powders.py` import endpoint and confirm ImportMode usage with skip/overwrite/merge
3. Confirm user-record protection: data_source=="manual" records are never overwritten
4. Confirm aliases endpoint exists at GET /powders/{id}/aliases
  </verify>
  <done>
Seed system loads all data from JSON fixtures with quality scoring and alias group application. GRT import endpoint supports 3-mode collision handling (skip/overwrite/merge) with user-record protection. Aliases endpoint returns linked powders.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add batch import endpoints for bullets and cartridges</name>
  <files>
    backend/app/api/bullets.py
    backend/app/api/cartridges.py
  </files>
  <action>
**Add `POST /bullets/import` endpoint** to `bullets.py`:

1. Import `ImportMode`, `ImportResult` from `app.schemas.powder` and `BulletImportRequest` from `app.schemas.bullet`.
2. Add endpoint:
```python
@router.post("/import", response_model=ImportResult)
async def import_bullets(
    data: BulletImportRequest,
    mode: ImportMode = Query(ImportMode.skip),
    db: AsyncSession = Depends(get_db),
):
    """Batch import bullets from JSON with collision handling.

    Modes: skip (ignore duplicates), overwrite (replace), merge (fill NULLs only).
    User-created records (data_source='manual') are NEVER overwritten -- the imported
    version gets renamed with ' (Import)' suffix.
    """
    # Pre-fetch existing bullets for collision detection
    result = await db.execute(select(Bullet))
    existing_map = {b.name.lower(): b for b in result.scalars().all()}

    created_count = 0
    updated_count = 0
    skipped = []
    errors = []

    for bullet_data in data.bullets:
        try:
            name_lower = bullet_data.name.lower()
            existing = existing_map.get(name_lower)
            dump = bullet_data.model_dump()

            if existing:
                if existing.data_source == "manual":
                    # Never overwrite user data -- create renamed copy
                    new_name = f"{bullet_data.name} (Import)"
                    bullet = Bullet(**{k: v for k, v in dump.items() if hasattr(Bullet, k)})
                    bullet.name = new_name
                    bullet.caliber_family = derive_caliber_family(bullet.diameter_mm)
                    b_dict = dump.copy()
                    breakdown = compute_bullet_quality_score(b_dict, bullet.data_source or "manufacturer")
                    bullet.quality_score = breakdown.score
                    db.add(bullet)
                    existing_map[new_name.lower()] = bullet
                    created_count += 1
                elif mode == ImportMode.skip:
                    skipped.append(bullet_data.name)
                elif mode == ImportMode.overwrite:
                    for key, value in dump.items():
                        if hasattr(existing, key) and key not in ("name",):
                            setattr(existing, key, value)
                    existing.caliber_family = derive_caliber_family(existing.diameter_mm)
                    b_dict = {c.key: getattr(existing, c.key) for c in Bullet.__table__.columns}
                    breakdown = compute_bullet_quality_score(b_dict, existing.data_source)
                    existing.quality_score = breakdown.score
                    updated_count += 1
                elif mode == ImportMode.merge:
                    for key, value in dump.items():
                        if hasattr(existing, key) and getattr(existing, key) is None and value is not None:
                            setattr(existing, key, value)
                    existing.caliber_family = derive_caliber_family(existing.diameter_mm)
                    b_dict = {c.key: getattr(existing, c.key) for c in Bullet.__table__.columns}
                    breakdown = compute_bullet_quality_score(b_dict, existing.data_source)
                    existing.quality_score = breakdown.score
                    updated_count += 1
            else:
                bullet = Bullet(**{k: v for k, v in dump.items() if hasattr(Bullet, k)})
                bullet.caliber_family = derive_caliber_family(bullet.diameter_mm)
                b_dict = dump.copy()
                breakdown = compute_bullet_quality_score(b_dict, bullet.data_source or "manufacturer")
                bullet.quality_score = breakdown.score
                db.add(bullet)
                existing_map[name_lower] = bullet
                created_count += 1
        except Exception as e:
            errors.append(f"{bullet_data.name}: {e}")

    await db.commit()
    return ImportResult(created=created_count, updated=updated_count, skipped=skipped, errors=errors)
```

IMPORTANT: Place this endpoint BEFORE the `GET /{bullet_id}` route to avoid FastAPI treating "import" as a UUID path parameter.

**Add `POST /cartridges/import` endpoint** to `cartridges.py` following the same pattern:

1. Import `ImportMode`, `ImportResult` from `app.schemas.powder` and `CartridgeImportRequest` from `app.schemas.cartridge`.
2. Add the endpoint mirroring the bullet import pattern but using Cartridge model, `compute_cartridge_quality_score`, and `CartridgeImportRequest`. Use `cartridge.groove_diameter_mm` for `derive_caliber_family()`.
3. Same collision handling: skip/overwrite/merge with user-record protection.
4. Place BEFORE `GET /{cartridge_id}` route.

Both endpoints should have rate limiting at the router level (they're under the default rate limit from slowapi, no special rate needed since bulk imports are infrequent).
  </action>
  <verify>
1. Read `bullets.py` and confirm POST /bullets/import exists with ImportMode, collision handling, user-record protection, and quality scoring
2. Read `cartridges.py` and confirm POST /cartridges/import exists with matching pattern
3. Confirm both endpoints are positioned BEFORE their respective GET /{id} routes
4. Confirm both import endpoints return ImportResult schema
  </verify>
  <done>
POST /bullets/import and POST /cartridges/import endpoints exist with 3-mode collision handling (skip/overwrite/merge), user-record protection (data_source="manual" never overwritten), automatic quality scoring, and caliber family derivation.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add comprehensive tests for import pipelines</name>
  <files>
    backend/tests/test_import_pipelines.py
  </files>
  <action>
Create `backend/tests/test_import_pipelines.py` with tests covering:

Follow the existing test patterns from `test_api_integration.py` (read it for: async client setup, conftest fixtures, aiosqlite backend, TestClient usage).

**Fixture loading tests (5-7 tests):**
1. `test_load_fixture_powders_valid_json` -- Verify `_load_fixture("powders.json")` returns a list with 200+ entries
2. `test_load_fixture_bullets_valid_json` -- Verify bullets.json has 100+ entries
3. `test_load_fixture_cartridges_valid_json` -- Verify cartridges.json has 50+ entries
4. `test_load_fixture_no_duplicate_powder_names` -- All powder names are unique
5. `test_load_fixture_no_duplicate_bullet_names` -- All bullet names are unique
6. `test_load_fixture_no_duplicate_cartridge_names` -- All cartridge names are unique
7. `test_load_fixture_alias_names_exist_in_powders` -- Every name in powder_aliases.json exists in powders.json

**Powder import endpoint tests (5-6 tests):**
8. `test_import_grt_skip_mode` -- Import with mode=skip: existing powder is skipped, new powder is created
9. `test_import_grt_overwrite_mode` -- Import with mode=overwrite: existing non-manual powder is updated
10. `test_import_grt_merge_mode` -- Import with mode=merge: only NULL fields on existing record are filled
11. `test_import_grt_manual_record_protection` -- Import overwrite against data_source="manual" record: creates renamed copy, does NOT modify original
12. `test_powder_aliases_endpoint` -- GET /powders/{id}/aliases returns linked powders when alias_group is set

**Bullet import endpoint tests (4-5 tests):**
13. `test_import_bullets_skip_mode` -- POST /bullets/import with mode=skip: creates new, skips existing
14. `test_import_bullets_overwrite_mode` -- Overwrites existing non-manual bullet
15. `test_import_bullets_manual_protection` -- Manual bullet is not overwritten, import creates renamed copy
16. `test_import_bullets_quality_scoring` -- Imported bullets have quality_score computed

**Cartridge import endpoint tests (3-4 tests):**
17. `test_import_cartridges_skip_mode` -- POST /cartridges/import with mode=skip
18. `test_import_cartridges_with_lineage` -- Imported cartridge has parent_cartridge_name preserved
19. `test_import_cartridges_manual_protection` -- Manual cartridge not overwritten

**Integration test:**
20. `test_seed_initial_data_loads_fixtures` -- Call seed_initial_data on empty DB, verify powder/bullet/cartridge counts match fixture file sizes

Use `pytest.mark.asyncio` for async tests. Use aiosqlite for the test database (NOT PostgreSQL) following the pattern in conftest.py. For GRT import tests, you may need to create a minimal .propellant XML fixture or mock the file upload.

For tests that need seed data, call `seed_initial_data(db)` directly rather than going through the API. For import endpoint tests, use the httpx AsyncClient with the FastAPI test app.

Note: pg_trgm fuzzy search won't work in aiosqlite tests -- that's fine, import tests don't need search. Just ensure all import endpoint tests use name-based collision detection which works with SQLite.
  </action>
  <verify>
Run `cd C:/Users/vall-/Desktop/projectes/simulador_balistica/backend && python -m pytest tests/test_import_pipelines.py -v` and confirm:
1. All tests pass (0 failures)
2. At least 18 tests exist
3. Test output shows fixture loading, import modes, user-record protection, and seed integration all covered
  </verify>
  <done>
18+ tests pass covering: fixture JSON validity and uniqueness, 3-mode collision handling for all three entity types, user-record protection, alias endpoint, quality scoring on import, and seed loading from fixtures. Zero failures.
  </done>
</task>

</tasks>

<verification>
1. All tests pass: `cd backend && python -m pytest tests/ -v` (existing 200+ tests still pass, plus new 18+ import tests)
2. Seed system loads from fixtures: `python -c "from app.seed.initial_data import _load_fixture; print(len(_load_fixture('powders.json')), 'powders')"` returns 200+
3. Import endpoints exist: POST /api/v1/powders/import-grt, POST /api/v1/bullets/import, POST /api/v1/cartridges/import
4. Aliases endpoint exists: GET /api/v1/powders/{id}/aliases
5. User-record protection: manual records are never overwritten in any import mode
</verification>

<success_criteria>
- seed_initial_data loads 200+ powders, 100+ bullets, 50+ cartridges from JSON fixtures on first boot
- GRT import endpoint supports mode=skip/overwrite/merge with user-record protection
- POST /bullets/import and POST /cartridges/import work with 3-mode collision handling
- GET /powders/{id}/aliases returns linked powders by alias_group
- Quality scores computed for all seeded and imported records
- 18+ new tests pass, zero failures across entire test suite
</success_criteria>

<output>
After completion, create `.planning/phases/05-import-pipelines-and-fixture-data/05-03-SUMMARY.md`
</output>
